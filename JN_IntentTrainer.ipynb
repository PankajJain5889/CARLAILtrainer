{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import itertools\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
    "tf.config.optimizer.set_jit(True) # Enable accelerated linear algebra \n",
    "import math\n",
    "from Helper.DataLoader import Loader , batchSize ,genBranch\n",
    "from Helper.ImitationLearning import Branches , BranchCommands ,create_network , dropoutVec\n",
    "from Helper.ImageAug import images_aug\n",
    "from tensorflow.core.protobuf import saver_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfromScratch =  True\n",
    "epochs = 1000\n",
    "MAX_LR_COUNTER = 3 # model has to perform worse for this number of cases to decrement learning rate \n",
    "memory_fraction = 0.5\n",
    "MAX_LEARNING_RATE = 2e-4\n",
    "MIN_LEARNING_RATE = 1e-7\n",
    "LEARNING_RATE_DECAY = 0.5\n",
    "LEARNING_RATE =  [MAX_LEARNING_RATE] * len(Branches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training points in Follow_Lane is 82322\n",
      "Validation points in Follow_Lane is 82322\n",
      "Training points in Left is 82322\n",
      "Validation points in Left is 82322\n",
      "Training points in Right is 82322\n",
      "Validation points in Right is 82322\n",
      "Training points in Straight is 82322\n",
      "Validation points in Straight is 82322\n",
      "Training points in Speed is 82322\n",
      "Validation points in Speed is 82322\n",
      "Training points in Intent is 82322\n",
      "Validation points in Intent is 82322\n",
      "steps_per_epoch:  1715\n",
      "VAL_STEPS: 1715\n"
     ]
    }
   ],
   "source": [
    "train_loader = Loader('/home/pankaj/CARLA_0.8.4/Collected_data/train/*/' ,'training_data',Branches , BranchCommands)\n",
    "val_loader = Loader('/home/pankaj/CARLA_0.8.4/Collected_data/train/*/' , 'validation_data',Branches , BranchCommands)\n",
    "dir_path = os.getcwd()\n",
    "contents= os.listdir(dir_path)\n",
    "model_path= os.path.join(dir_path, 'models')\n",
    "logs_path= os.path.join(dir_path ,'logs')\n",
    "if 'models' not in contents:\n",
    "    os.mkdir(model_path)\n",
    "if 'logs'not in contents:\n",
    "    os.mkdir(logs_path)\n",
    "\n",
    "for branch in Branches:#,'speed', 'intent'\n",
    "    print(f\"Training points in {branch} is {train_loader.dict[branch]['Count']}\")\n",
    "    print(f\"Validation points in {branch} is {val_loader.dict[branch]['Count']}\")\n",
    "\n",
    "total_train = sum(train_loader.dict[branch]['Count'] for branch in Branches if branch not in [\"Speed\" , \"Intent\"])\n",
    "total_val =  sum(val_loader.dict[branch]['Count'] for branch in Branches if branch not in [\"Speed\" , \"Intent\"] )\n",
    "steps_per_epoch = total_train//(batchSize * len(Branches))\n",
    "print(\"steps_per_epoch: \",steps_per_epoch)\n",
    "VAL_STEPS = total_val//(batchSize*len(Branches))\n",
    "print(\"VAL_STEPS:\", VAL_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchListGenTrain = []\n",
    "batchListGenVal = []\n",
    "for branch in Branches:\n",
    "    miniBatchGen = genBranch(branch = train_loader.dict[branch],command = branch ,batchSize = batchSize)\n",
    "    batchListGenTrain.append(miniBatchGen)\n",
    "    miniBatchGen = genBranch(branch = val_loader.dict[branch],command = branch, batchSize = batchSize)\n",
    "    batchListGenVal.append(miniBatchGen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from IPython.display import clear_output\n",
    "while True:\n",
    "    for j in range(len(Branches)):\n",
    "        xs , ys = next(batchListGenTrain[j])\n",
    "        #xs = images_aug(xs)\n",
    "        xs = np.multiply(xs , 1.0/255.0)\n",
    "        command = np.eye(len(Branches))[ys[0,24].astype(np.int8)].reshape(1,-1)\n",
    "        for i in range(batchSize):\n",
    "            plt.imshow(xs[i])\n",
    "            plt.show()\n",
    "            print(f\"Steer: {ys[i][0]} Throttle: {ys[i][1]} Brake: {ys[i][2]} TimeStamp: {ys[i][11] }\")\n",
    "            print(f\"Speed: {ys[i][10]} Directions: {ys[i][24]} Branch:{Branches[j]} Iterator: {i}\") \n",
    "            print(f\"Ped: {ys[i][25]} Veh: {ys[i][26]} Tra: {ys[i][27]} Command:{command}\")\n",
    "            clear_output(wait = True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pankaj/CARLA_0.8.4/Trainer_module/Helper/ImitationLearning.py:228: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pankaj/CARLA_0.8.4/Trainer_module/Helper/ImitationLearning.py:41: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pankaj/CARLA_0.8.4/Trainer_module/Helper/ImitationLearning.py:110: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/pankaj/CARLA_0.8.4/Trainer_module/Helper/ImitationLearning.py:122: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.\n",
      "\n",
      "Tensor(\"Network/Branch_0/fc_8:0\", shape=(?, 3), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"Network/Branch_1/fc_11:0\", shape=(?, 3), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"Network/Branch_2/fc_14:0\", shape=(?, 3), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"Network/Branch_3/fc_17:0\", shape=(?, 3), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"Network/Branch_4/fc_20:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"Network/Branch_5/fc_23:0\", shape=(?, 3), dtype=float32, device=/device:GPU:0)\n",
      "WARNING:tensorflow:From /home/pankaj/CARLA_0.8.4/Trainer_module/Helper/ImitationLearning.py:255: The name tf.squared_difference is deprecated. Please use tf.math.squared_difference instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pankaj/CARLA_0.8.4/Trainer_module/Helper/ImitationLearning.py:266: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pankaj/CARLA_0.8.4/Trainer_module/Helper/ImitationLearning.py:269: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pankaj/CARLA_0.8.4/Trainer_module/Helper/ImitationLearning.py:270: The name tf.train.experimental.enable_mixed_precision_graph_rewrite is deprecated. Please use tf.compat.v1.train.experimental.enable_mixed_precision_graph_rewrite instead.\n",
      "\n",
      "WARNING:tensorflow:You already have existing Sessions that do not use mixed precision. enable_mixed_precision_graph_rewrite() will not affect these Sessions.\n",
      "Starting epoch: 0\n",
      "Running Validation\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'branchConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4b71a9670481>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAL_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mstep_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBranches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranchConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                     \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchListGenVal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'branchConfig' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup tensorflow \n",
    "tf.reset_default_graph()\n",
    "sessGraph = tf.Graph()\n",
    "# use many gpus\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = memory_fraction \n",
    "with sessGraph.as_default():\n",
    "    sess = tf.Session(graph=sessGraph, config=config)\n",
    "    with sess.as_default():\n",
    "        nettensors = create_network()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # merge all summaries into a single op\n",
    "        merged_summary_op = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver(write_version=saver_pb2.SaverDef.V2)\n",
    "        if not (trainfromScratch):\n",
    "            print(\"loading base model from \" , model_path)\n",
    "            saver.restore(sess, model_path+\"/model.ckpt\")  # restore trained parameters   \n",
    "        min_epoch_loss = np.array([[float('inf')]*len(Branches)])\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, graph=sessGraph)\n",
    "        tboard_counter = 0\n",
    "        lr_counter = [0] * len(Branches)\n",
    "        for epoch in range(epochs): #1st loop for epochs \n",
    "            start_time=time.time()\n",
    "            print(f'Starting epoch: {epoch}')\n",
    "            #epoch_loss=0\n",
    "            for step in range(steps_per_epoch):# second loop for each step in a epoch\n",
    "                #step_loss=0\n",
    "                for j in range(len(Branches)):# each step will update all braches one at a time  \n",
    "                    xs , ys = next(batchListGenTrain[j])\n",
    "                    if step%100 == 0:\n",
    "                        xs = images_aug(xs)\n",
    "                    xs = np.multiply(xs , 1.0/255.0)\n",
    "                    command = np.eye(len(Branches))[ys[0,24].astype(np.int8)].reshape(1,-1)\n",
    "                    contSolver = nettensors['optimizers']\n",
    "                    contLoss = nettensors['losses']\n",
    "                    log = nettensors['Logger']\n",
    "                    feedDict = {nettensors['inputs'][0]: xs, \n",
    "                                nettensors['inputs'][1][0]: command,\n",
    "                                nettensors['inputs'][1][1]:ys[:,10].reshape([batchSize,1]),\n",
    "                                nettensors['droput']: dropoutVec, \n",
    "                                nettensors['targets'][0]: ys[:,10].reshape([batchSize,1]),\n",
    "                                nettensors['targets'][1]: ys[:,0:3],\n",
    "                                nettensors['targets'][2]: ys[:,25:28] ,\n",
    "                                nettensors['learning_rate']: LEARNING_RATE[j]\n",
    "                               }  #\n",
    "                    _,loss,log    = sess.run([contSolver, contLoss, log ], feed_dict = feedDict)\n",
    "                    #print(log)\n",
    "                #time.sleep(20)\n",
    "                summary = merged_summary_op.eval(feed_dict=feedDict)\n",
    "                summary_writer.add_summary(summary, tboard_counter)\n",
    "                tboard_counter+=1\n",
    "            print(\"Running Validation\")\n",
    "            epoch_loss = np.zeros((1,len(Branches)))\n",
    "            for step in range(VAL_STEPS):\n",
    "                step_loss = [0]*len(Branches) \n",
    "                for j in range(len(branchConfig)):\n",
    "                    xs, ys = next(batchListGenVal[j])\n",
    "                    xs =  np.multiply(xs , 1.0/255.0)\n",
    "                    contLoss = nettensors['losses'] \n",
    "                    log = nettensors['Logger']\n",
    "                    command = np.eye(len(Branches))[ys[0,24].astype(np.int8)].reshape(1,-1)\n",
    "                    feedDict = {\n",
    "                            nettensors['inputs'][0]: xs, \n",
    "                            nettensors['inputs'][1][0]: command,\n",
    "                            nettensors['inputs'][1][1]:ys[:,10].reshape([batchSize,1]),\n",
    "                            nettensors['droput']:[1] * len(dropoutVec),  \n",
    "                            nettensors['targets'][0]: ys[:,10].reshape([batchSize,1]),\n",
    "                            nettensors['targets'][1]: ys[:,0:3],\n",
    "                            nettensors['targets'][2]: ys[:,25:28],\n",
    "                            }  \n",
    "                    loss,log = sess.run([contLoss , log], feed_dict = feedDict)\n",
    "                    #print(f\"Validation--> Step:: {step}  Branch: {branch_map[j]} loss: {loss}\" )\n",
    "                    #print(log)\n",
    "                    step_loss[j] = loss \n",
    "                epoch_loss+=step_loss \n",
    "            epoch_loss /= VAL_STEPS    \n",
    "            branchImprovement = list((epoch_loss < min_epoch_loss)[0])\n",
    "            print(f\"Epoch no. {epoch} took {(time.time() - start_time)//60} minutes\")\n",
    "            print(f\"branch improvement: {branchImprovement}\")\n",
    "            print(f\"branch losses:{epoch_loss}\")\n",
    "            print(f\"Minimum epoch loss: {min_epoch_loss}\")\n",
    "            if np.sum(epoch_loss < min_epoch_loss) > len(Branches)/2:# Loss has decreased in more than half the branches\n",
    "                min_epoch_loss = epoch_loss               \n",
    "                print(f\"Found better model saving  checkpoint\")\n",
    "                checkpoint_path=os.path.join(model_path , \"model.ckpt\")\n",
    "                file_name= saver.save(sess , checkpoint_path)\n",
    "                for j , imp in enumerate(branchImprovement):\n",
    "                    if imp:\n",
    "                        lr_counter[j] = 0                       \n",
    "            else: # Did not find a better model\n",
    "                for j , imp in enumerate(branchImprovement):\n",
    "                    if not imp:\n",
    "                        lr_counter[j] += 1 # Increment counter only for which improvement was not found    \n",
    "            for j , imp in enumerate(branchImprovement):            \n",
    "                if lr_counter[j] ==  MAX_LR_COUNTER:\n",
    "                    LEARNING_RATE[j] *= LEARNING_RATE_DECAY\n",
    "                    if LEARNING_RATE[j] <= MIN_LEARNING_RATE:\n",
    "                        print(f\"Last learning rate achieved for {Branches[j]}\")\n",
    "                        LEARNING_RATE[j] = MAX_LEARNING_RATE\n",
    "                    print(f\"Updated learning rate for {Branches[j] }: {LEARNING_RATE[j]} \", )\n",
    "                    lr_counter[j] = 0\n",
    "            print(f\"lr counter : {lr_counter}\")\n",
    "            print(f\"Current learning rate : {LEARNING_RATE}\")             \n",
    "        print(\"Saving last model\")\n",
    "        checkpoint_path=os.path.join(model_path , \"model.ckpt\")\n",
    "        file_name= saver.save(sess , checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
